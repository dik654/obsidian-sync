# 뉴턴법 유도

  

## 경사하강법과의 공통점

  

둘 다 `x_new = x + Δx`로 x를 갱신한다. **Δx를 어떻게 정하느냐**만 다르다.

  

| | 경사하강법 | 뉴턴법 |

|---|---|---|

| Δx | -η · f'(x) | -f'(x) / f''(x) |

| lr 필요 | O (사람이 정함) | X (f''가 대신함) |

  

## 1단계: 테일러 전개 (2차)

  

경사하강법은 1차 근사를 썼다:

  

```

f(x + Δx) ≈ f(x) + f'(x)·Δx ← 1차 (경사하강법)

```

  

뉴턴법은 **2차 항까지** 쓴다:

  

```

f(x + Δx) ≈ f(x) + f'(x)·Δx + ½·f''(x)·Δx² ← 2차 (뉴턴법)

```

  

½은 테일러 전개 일반 공식 `f^(n)(x) / n!`에서 n=2일 때 2! = 2에서 나온 것이다.

  

2차까지 쓰면 곡선의 **휘어짐(곡률)**까지 반영하므로 더 정확하다.

  

## 2단계: 경사하강법은 왜 lr이 필요했나?

  

경사하강법의 1차 근사 `f(x) + f'(x)·Δx`는 직선이다.

직선에는 최솟값이 없다 (끝없이 내려감). 그래서 **Δx를 얼마나 갈지** 사람이 lr로 정해줘야 했다.

  

```

1차 근사 (직선): 2차 근사 (포물선):

\ \ /

\ \ /

\ \_/ ← 최솟값이 있다!

\

(최솟값 없음)

```

  

## 3단계: 2차 근사에서 최솟값 찾기

  

2차 근사 `f(x) + f'(x)·Δx + ½·f''(x)·Δx²`는 포물선이다.

포물선에는 최솟값이 있으므로, **미분해서 0으로 놓으면** Δx가 바로 나온다:

  

```

d/dΔx [f(x) + f'(x)·Δx + ½·f''(x)·Δx²] = 0

```

  

미분하면:

  

```

f'(x) + f''(x)·Δx = 0

```

  

Δx에 대해 풀면:

  

```

Δx = -f'(x) / f''(x)

```

  

**lr이 필요 없다.** f''(x)가 자동으로 보폭을 결정해준다.

  

## 4단계: 갱신 규칙

  

```

x_new = x + Δx = x - f'(x) / f''(x)

```

  

이것이 뉴턴법의 전부다.

  

## 왜 곡률(f'')을 알면 더 빠르게 수렴하는가?

  

기울기만 알면 **방향**은 알지만 **얼마나 갈지** 모른다.

  

```

경우 A: 급하게 휘어짐 (f'' 큼) | 경우 B: 완만하게 휘어짐 (f'' 작음)

  

 \                /          \                         /

   \            /              \                     /

     \        /                  \                 /

       \    /                      \      ______ / ← 이 부분: 바닥이 넓고 평평함

         \/ ← 최솟값이 가까움            \__/ ← 최솟값이 멀리 있음

```

  

**둘 다 기울기(f')는 같을 수 있다.** 그런데:

  

- 경우 A: 급하게 휘어짐 → 최솟값이 **가까움** → 조금 가야 함

- 경우 B: 완만하게 휘어짐 → 최솟값이 **멀리** 있음 → 많이 가야 함

  

경사하강법은 기울기만 보니까 A와 B를 **구분 못한다.** lr을 고정해놓고 똑같은 보폭으로 간다.

  

뉴턴법은 곡률(f'')을 알기 때문에:

  

```

Δx = -f'(x) / f''(x)

  

경우 A: f'' 큼 → Δx 작음 → 조금 감 ✓

경우 B: f'' 작음 → Δx 큼 → 많이 감 ✓

```

  

**매 지점마다 최적의 보폭을 자동으로 계산**하니까 빠르다.

경사하강법이 눈 감고 걷는 거라면, 뉴턴법은 지형을 보고 걷는 것이다.

  

## 왜 f''(x)가 lr 역할을 하는가?

  

`Δx = -f'(x) / f''(x)`를 경사하강법과 비교하면:

  

```

경사하강법: Δx = -η · f'(x) η = 사람이 정한 상수

뉴턴법: Δx = -1/f''(x) · f'(x) 1/f''(x) = 자동 계산된 "lr"

```

  

- f''(x)가 **크다** (급격히 휘어짐) → 1/f''(x) 작음 → 조금 이동

- f''(x)가 **작다** (완만히 휘어짐) → 1/f''(x) 큼 → 크게 이동

  

곡률에 맞춰 자동으로 보폭이 조절된다.

  

## step29 코드와 대응

  

```

f(x) = x⁴ - 2x²

f'(x) = 4x³ - 4x ← backward()로 자동 계산

f''(x) = 12x² - 4 ← gx2()로 수동 계산 (이후 step에서 자동화)

```

  

```rust

// x = x - f'(x) / f''(x)

let grad = x.grad().unwrap(); // f'(x)

x.set_data(x.data() - &grad / &gx2(&x.data())); // f''(x)로 나눔

```

  

결과: 경사하강법은 1000회에도 미수렴, 뉴턴법은 **6회에 수렴**.

  

## 3차, 4차는 왜 안 쓰나?

  

이론상 테일러 전개를 3차, 4차로 확장할 수 있지만, 실전에서는 2차까지만 쓴다.

  

이유는 **비용 대비 효과**가 급격히 나빠지기 때문이다.

변수가 n개일 때 각 차수에서 저장해야 하는 도함수 정보의 크기:

  

| 차수 | 필요한 정보 | 저장 공간 |

|---|---|---|

| 1차 (경사하강법) | f' (기울기 벡터) | n |

| 2차 (뉴턴법) | f'' (헤시안 행렬) | n² |

| 3차 | f''' (3차 텐서) | n³ |

| 4차 | f'''' (4차 텐서) | n⁴ |

  

각 차수가 뜻하는 것 (자동차 비유):

  

| 차수 | 의미 | 비유 |

|---|---|---|

| f'(x) | 기울기 | 속도 |

| f''(x) | 곡률 (기울기의 변화율) | 가속도 |

| f'''(x) | 곡률의 변화율 | 저크 (급출발/급정지의 불쾌감) |

| f''''(x) | 그 변화의 변화 | 스냅 |

  

예를 들어 변수가 1000만 개(10⁷)인 신경망이면:

  

- 1차: 10⁷개 (수십 MB) → 실용적

- 2차: 10¹⁴개 (수백 TB) → 이미 비현실적

- 3차: 10²¹개 → 불가능

  

그래서 실전 딥러닝은 **1차(경사하강법 계열)**을 쓰되,

2차 정보를 **근사**해서 효율을 올리는 방향으로 발전했다 (Adam 등).

  

## 요약

  

```

경사하강법: 1차 근사 → 직선 → 최솟값 없음 → lr을 사람이 정함

뉴턴법: 2차 근사 → 포물선 → 최솟값 있음 → 곡률로 보폭 자동 결정 → 빠르게 수렴

3차 이상: 정확하지만 저장 비용이 n³, n⁴... → 비현실적

```