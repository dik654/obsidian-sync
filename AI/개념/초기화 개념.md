## Lazy Initialization (지연 초기화)

### 왜 필요한가?

신경망을 설계할 때, 각 레이어의 **출력 크기**는 설계자가 정하지만 **입력 크기**는 이전 레이어의 출력에 의해 결정된다.

```
입력(1) → [Linear 10] → [Linear 1] → 출력(1)
           in=? out=10   in=? out=1
```

- `Linear(10)` 생성 시점: 출력 10은 알지만, 입력이 1인지 100인지 모름
- 첫 `forward(x)` 호출 시: `x.shape = (100, 1)` → **입력 크기 = 1** 확정 → 그때 W 생성

### 장점

|방식|코드|
|---|---|
|지연 초기화 없음|`Linear(1, 10)`, `Linear(10, 1)` — 입출력 크기 둘 다 지정|
|지연 초기화|`Linear(10)`, `Linear(1)` — 출력 크기만 지정|

레이어가 많아질수록 입력 크기를 일일이 맞추는 것은 실수하기 쉽다. 지연 초기화는 이 부담을 없애준다.

---

## Xavier 초기화

### 문제: 왜 초기값이 중요한가?

각 레이어의 출력은 입력들의 **가중합**이다:

$$y = x_1 w_1 + x_2 w_2 + \cdots + x_n w_n$$

입력이 $n$개이고 각 $x_i$와 $w_i$가 독립이면, 출력의 분산은:

$$\text{Var}(y) = n \cdot \text{Var}(x) \cdot \text{Var}(w)$$

> $n$개의 독립적인 곱의 합이므로, 각 항의 분산이 합쳐진다.

### $w$의 초기값에 따른 결과

|초기화|$\text{Var}(w)$|$\text{Var}(y)$|문제|
|---|---|---|---|
|`0.01 * randn`|$0.0001$|$n \times 0.0001$|층을 지날수록 신호가 **0에 수렴** (소실)|
|`1.0 * randn`|$1.0$|$n$|층을 지날수록 신호가 **폭발**|
|Xavier|$\frac{1}{n}$|$n \times \frac{1}{n} = 1$|분산이 **1로 유지**|

### Xavier 초기화 공식

$$w \sim \mathcal{N}\left(0,;\frac{1}{n_{\text{in}}}\right)$$

구현: 표준정규분포에서 뽑은 값에 $\sqrt{\frac{1}{n_{\text{in}}}}$을 곱한다.

```
w = randn(in_size, out_size) * sqrt(1 / in_size)
```

### 구체적 예시

|레이어|$n_{\text{in}}$|scale = $\sqrt{1/n_{\text{in}}}$|의미|
|---|---|---|---|
|1층 (입력 1 → 은닉 10)|1|$1.0$|입력이 1개라 축소 불필요|
|2층 (은닉 10 → 출력 1)|10|$\approx 0.316$|10개 입력의 합이니 각 가중치를 작게|
|만약 은닉 100개라면|100|$0.1$|100개 입력 → 더 작게|

> **핵심**: 입력 뉴런 수가 많을수록 각 가중치를 작게 만들어서, 합산 결과의 분산을 1로 유지한다.